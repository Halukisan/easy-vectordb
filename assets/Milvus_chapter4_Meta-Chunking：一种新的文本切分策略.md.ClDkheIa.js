import{_ as t,c as l,o as e,ag as a,j as s,a as i}from"./chunks/framework.DUcTBpf6.js";const r="/easy-vectordb/images/metac.png",p="/easy-vectordb/images/RAGyinru.png",o="/easy-vectordb/images/PPLhou.png",h="/easy-vectordb/images/metachunkingpic.png",m=JSON.parse('{"title":"Meta-Chunking","description":"","frontmatter":{},"headers":[],"relativePath":"Milvus/chapter4/Meta-Chunking：一种新的文本切分策略.md","filePath":"Milvus/chapter4/Meta-Chunking：一种新的文本切分策略.md","lastUpdated":1756795290000}'),k={name:"Milvus/chapter4/Meta-Chunking：一种新的文本切分策略.md"};function g(d,n,$,c,_,u){return e(),l("div",null,[...n[0]||(n[0]=[a('<h1 id="meta-chunking" tabindex="-1">Meta-Chunking <a class="header-anchor" href="#meta-chunking" aria-label="Permalink to &quot;Meta-Chunking&quot;">​</a></h1><p>该章节大部分内容将围绕着论文中提到的新技术和架构设计展开，FusionANNS部分有一个Demo代码，但并没有调用GPU，也没有充分的利用SSD进行存储。主要原因是架构设计过于庞大难以实现，属于整体项目架构方面，但本文中介绍的Meta-Chunking框架，在RAG系统中是极为重要的，数据分块的质量将直接影响到问答系统的召回率。低效的分块策略会导致上下文不完整或包含过多无关信息，从而损害问答系统的性能。</p><p>但在RAG的流程中，文本分块往往是容易被忽视的关键环节，这篇论文中提出Meta-Chunking这一元分块框架，<strong>通过识别最优分割点与保留全局信息的双重策略，专门提升分块质量</strong>。</p><p><img src="'+r+'" alt="Refer to caption"></p><p>如图 1 所示，Meta-Chunking融合了传统文本分块策略的优势，例如遵循预设的分块长度约束和确保句子结构完整性，同时增强了在分块过程中保证逻辑连贯性的能力。我们将通过分块获得的每个文本块称为元块（Meta-Chunk），它由段落中按顺序排列的句子集合组成。这些句子不仅具有语义相关性，更重要的是还包含深层次的语言逻辑联系，包括但不限于总分、并列、递进和例证等关系。 该方法基于一个核心理念：通过允许分块大小的可变性，更有效地捕捉并保持内容的逻辑完整性。这种动态调整的粒度控制确保每个分割后的块都包含完整且独立的思想表达，从而避免分割过程中出现逻辑链条断裂。这不仅提升了文档检索的相关性，同时也增强了内容的清晰度。</p><p>首先突破基于相似度的分块局限，我们利用LLM的能力，设计了两种基于不确定性的自适应分块技术<strong>Perplexity Chunking（PPL）</strong> and <strong>Margin Sampling Chunking（MSP)</strong> 针对不同文本的固有复杂性，我们通过动态合并实现元分块，在细粒度与粗粒度文本分块间取得平衡。此外，我们建立了全局信息补偿机制，包含两阶段层次化摘要生成流程与聚焦缺失反思、精修和补全的三阶段文本块重写过程。这些组件共同增强了文本块的语义完整性与上下文连贯性。</p><p>对于一些论文中提到的专业术语，我们进行了翻译和解释：</p><ul><li><p>Through lightweight chunking algorithm design, the logical analysis capability of LLMs is decoupled into computable the PPL features and MSP indicators, achieving identification of textual logical boundaries and dynamic balance of chunking granularity.</p><p>• 通过轻量级分块算法设计，将 LLMs 的逻辑分析能力解耦为可计算的 PPL 特征和 MSP 指标，实现文本逻辑边界的识别与分块粒度的动态平衡。</p></li><li><p>We establish a information compensation mechanism that collaboratively executes through a three-stage missing-aware rewriting process and a two-stage context-aware summary generation, repairing the semantic discontinuities in text chunks.</p><p>• 我们建立了信息补偿机制，通过三阶段缺失感知重写流程与两阶段上下文感知摘要生成的协同执行，修复文本块中的语义断裂问题。</p></li></ul><p>下面将正式进入论文的详细介绍部分，首先你需要了解文章中提出的两个非常重要的概念，同时也是论文提出的分块的两种策略：</p><blockquote><ol><li><strong>PPL (Perplexity)</strong>：困惑度是自然语言处理中一个常用的评价指标，用于衡量一个概率模型预测样本的能力。简单来说，它评估了语言模型在预测下一个词时的不确定性。较低的困惑度意味着模型对文本的理解更好，能够更准确地预测下一个词。因此，在轻量级分块算法设计中，PPL 特征可能被用来作为衡量文本逻辑连贯性的一个标准，以帮助识别文本的逻辑边界。</li><li><strong>MSP (Most Significant Passage)</strong>：虽然“MSP”这个缩写在特定领域可能有多种含义，但在这里，我们可以假设它指的是“最重要段落”或“最具代表性片段”。在给定的语境下，MSP 指标可能是用于识别文本中最能代表其主要内容或主题的部分。通过计算每个分块的重要性或代表性，可以帮助实现分块粒度的动态平衡，确保每个分块既不过于细碎也不过于笼统，从而更好地保持原始文本的逻辑结构和信息完整性。</li></ol></blockquote><h3 id="策略一-perplexity-calculation-per-sentence-ppl困惑度" tabindex="-1">策略一 Perplexity Calculation per Sentence （PPL困惑度） <a class="header-anchor" href="#策略一-perplexity-calculation-per-sentence-ppl困惑度" aria-label="Permalink to &quot;策略一 Perplexity Calculation per Sentence （PPL困惑度）&quot;">​</a></h3><p><img src="'+p+'" alt="alt text"> 如上图所示，根据PPL困惑度来处理文本分块的效果将比传统的固定分块以及文本相似度分块的要好。 PPL（困惑度，Perplexity）是衡量语言模型预测能力的一个指标，用于量化模型对文本的预测难度。用一句话来说：给定一个上下文T，对于T的下一个词 $t_i$，PPL表示大语言模型（LLM）在T的基础上，对 $t_i$ 的预测难度。在论文提出的Meta - Chunking框架中，基于模型计算PPL并实现文本分块的具体流程可分为预处理、PPL计算、边界识别、动态调整四个核心步骤，每个步骤都与模型的能力深度绑定，具体实现细节如下：</p><h3 id="_1-预处理-文本拆分与序列构建" tabindex="-1">1. 预处理：文本拆分与序列构建 <a class="header-anchor" href="#_1-预处理-文本拆分与序列构建" aria-label="Permalink to &quot;1. 预处理：文本拆分与序列构建&quot;">​</a></h3><ul><li><strong>句子拆分</strong>：将原始文本按标点符号（如句号、问号）拆分为独立句子，形成句子序列 $(x_1, x_2, ..., x_n)$。例如，将“动物迁徙由多种因素驱动。气候周期性变化影响最大。”拆分为 $x_1=$“动物迁徙由多种因素驱动”、$x_2=$“气候周期性变化影响最大”。</li><li><strong>上下文准备</strong>：为每个句子 $x_i$ 构建其“前文语境”，即包含 $x_i$ 之前的所有句子（$x_1$ 到 $x_{i-1}$）的token序列 $t_{\\lt i}$，作为模型预测的上下文输入。目标是将这些句子划分为若干语义连贯的块 $(X_1, X_2, ..., X_k)$。例如，当拆分为 $x_1=$“动物迁徙由多种因素驱动”、$x_2=$“气候周期性变化影响最大”时， $x_2$ 的前文语境 $t_{\\lt 2}$ 就是 $x_1$ 的所有token。</li></ul><h3 id="_2-ppl计算-模型预测与概率聚合" tabindex="-1">2. PPL计算：模型预测与概率聚合 <a class="header-anchor" href="#_2-ppl计算-模型预测与概率聚合" aria-label="Permalink to &quot;2. PPL计算：模型预测与概率聚合&quot;">​</a></h3><ul><li><p><strong>模型输入构造</strong>：对于句子 $x_i$ 中的每个token $t_k^i$（如“气候”“影响”等），模型的输入为“前文语境 $t_{\\lt i}$ + 句子 $x_i$ 中该token之前的所有token $t_{\\lt k}^i$”。例如，计算 $x_2$ 中“影响”的概率时，输入为“动物迁徙由多种因素驱动。气候周期性变化”。</p></li><li><p><strong>条件概率预测</strong>：模型（如Qwen2-7B、Baichuan2-7B等，为了效果，你可以选择参数量更大的模型，但我们用的只是大模型所提供的大参数量和性能，而不是让模型直接告诉你在哪里分块）基于输入的上下文，预测每个token $t_k^i$ 出现的条件概率 $P(t_k^i | t_{\\lt k}^i, t_{\\lt i})$。能力越强的模型，越能根据逻辑关联输出合理概率（如“气候”后接“影响”的概率高于接“电脑”）。 所以，这里模型最终输出的是什么呢？你可以先往下翻找，下文 PPL 的计算算法基础 这一部分里面的开头写道：</p><blockquote><p>PPL 的计算核心基于交叉熵。公式为 $PPL = 2^{H}$，其中交叉熵 $H = -\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 p(w_i | w_1, w_2, ..., w_{i-1})$，$N$ 表示测试集中单词总数，$p(w_i | w_1, w_2, ..., w_{i-1})$ 表示给定前面所有词的情况下第 $i$ 个词的条件概率。</p></blockquote><p>其中，$p(w_i | w_1, w_2, ..., w_{i-1})$ 就是模型最后输出的内容。</p></li><li><p><strong>单句PPL聚合</strong>：对句子 $x_i$ 中所有token的概率取平均值，得到该句子的PPL值：<br> [ PPL_M(x_i) = \\frac{\\sum_{k=1}^{K} PPL_M(t_k^i | t_{&lt;k}^i, t_{&lt;i})}{K} ]<br> 其中，$K$ 为句子 $x_i$ 的token总数，PPL值越低表示模型对该句子的预测越“确定”（与前文逻辑越连贯）。</p></li></ul><h3 id="_3-边界识别-基于ppl序列的分割点定位" tabindex="-1">3. 边界识别：基于PPL序列的分割点定位 <a class="header-anchor" href="#_3-边界识别-基于ppl序列的分割点定位" aria-label="Permalink to &quot;3. 边界识别：基于PPL序列的分割点定位&quot;">​</a></h3><ul><li><strong>PPL序列生成</strong>：将所有句子的PPL值按顺序排列，形成序列 $PPL_{seq} = (PPL(x_1), PPL(x_2), ..., PPL(x_n))$。例如，若 $x_1$ 到 $x_3$ 的PPL值为5.2、3.1、6.8，则序列为 $(5.2, 3.1, 6.8)$。</li><li><strong>最小值点检测</strong>：算法聚焦于PPL序列中的两类最小值点（潜在分割点）： <ol><li>某点两侧的PPL值均高于该点，且至少一侧差值超过阈值 $\\theta$（如 $x_2$ 的PPL=3.1，左侧 $x_1=5.2$、右侧 $x_3=6.8$，若 $\\theta=2$，则 $x_2$ 是分割点）；</li><li>左侧PPL值与该点的差值超过 $\\theta$，且右侧PPL值等于该点（如 $x_2=3.1$，左侧差值5.2-3.1=2.1&gt;θ，右侧 $x_3=3.1$）。</li></ol></li><li><strong>分割逻辑</strong>：这些最小值点表明模型对该句子的预测“突然变容易”，暗示其与前后文的逻辑关联较弱，因此将其作为分块边界。例如，上述序列中 $x_2$ 是分割点，分块结果为 $(x_1), (x_2, x_3)$。</li></ul><h3 id="_4-动态调整-解决长文本与粒度平衡问题" tabindex="-1">4. 动态调整：解决长文本与粒度平衡问题 <a class="header-anchor" href="#_4-动态调整-解决长文本与粒度平衡问题" aria-label="Permalink to &quot;4. 动态调整：解决长文本与粒度平衡问题&quot;">​</a></h3><ul><li><strong>KV缓存机制</strong>：当文本过长（超出模型上下文长度或GPU内存限制）时，算法会动态移除前文部分文本的键值对（KV pairs），在不显著损失上下文连贯性的前提下，确保PPL计算可执行。例如，处理1000句长文本时，仅保留最近200句的KV缓存用于后续句子的PPL计算。</li><li><strong>动态合并策略</strong>：先通过PPL Chunking得到“元块”（meta-chunk，即逻辑完整的句子组），再根据用户指定的chunk长度 $L$ 合并相邻元块。例如，若元块长度分别为 $c_1=50$、$c_2=40$、$c_3=30$，且 $L=110$，则合并 $c_1+c_2+c_3=120$（接近 $L$）作为最终分块。</li></ul><p>比如，对于存在一般-特殊（$x \\supset y$）、并列（$x | y$）、递进（$x \\to y$）等逻辑关系的句子序列，PPL值呈现逐渐下降的趋势，说明句子间逻辑连贯，应被划分为同一分块；而若某句子的PPL值突然升高，则表明其与前文逻辑关联较弱，需分割。<br> 例如：</p><ul><li>句子序列“动物迁徙由多种因素驱动。气候周期性变化影响最大，其次是食物资源分布。基因特征也起重要作用。”的PPL值依次为5、4.37、3.33，呈现下降趋势，应合并为一个块；</li><li>若中间插入一句PPL值为8的句子（如“汽车产业的发展依赖技术创新”），则该句与前后文逻辑脱节，PPL值显著升高，应作为分割点。</li><li>再以句子序列“小明喜欢运动。他每天早上跑步。天空是蓝色的。”为例： <ol><li>模型计算 $x_1$（小明喜欢运动）的PPL时，因无前文，PPL值较高（如6.5）；</li><li>计算 $x_2$（他每天早上跑步）时，模型基于“小明喜欢运动”的上下文，预测“跑步”等词的概率高，PPL值低（如3.2）；</li></ol></li></ul><h3 id="ppl-与模型的关系及模型在-ppl-计算中的作用" tabindex="-1">PPL 与模型的关系及模型在 PPL 计算中的作用 <a class="header-anchor" href="#ppl-与模型的关系及模型在-ppl-计算中的作用" aria-label="Permalink to &quot;PPL 与模型的关系及模型在 PPL 计算中的作用&quot;">​</a></h3><p>PPL（困惑度）是衡量语言模型预测能力的指标，有人可能会疑惑：PPL 只是通过算法实现的，没用到大模型的能力吗？其实不是的，PPL 的计算本身基于算法，但在实际应用，尤其是与大模型相关的场景下，会借助大模型的能力。</p><h4 id="ppl-的计算算法基础" tabindex="-1">PPL 的计算算法基础 <a class="header-anchor" href="#ppl-的计算算法基础" aria-label="Permalink to &quot;PPL 的计算算法基础&quot;">​</a></h4><p>PPL 的计算核心基于交叉熵。公式为 $PPL = 2^{H}$，其中交叉熵 $H = -\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 p(w_i | w_1, w_2, ..., w_{i-1})$，$N$ 表示测试集中单词总数，$p(w_i | w_1, w_2, ..., w_{i-1})$ 表示给定前面所有词的情况下第 $i$ 个词的条件概率。在计算时，需要准备未参与模型训练的文本数据，利用已训练好的模型计算每个词的条件概率，再求解交叉熵，最终得到 PPL。从这个计算流程看，PPL 计算依赖的是概率计算和数学公式，是算法层面的实现。在涉及主题建模（如 LDA 主题模型）时，困惑度可通过语料似然值间接计算，依赖于文档的主题分布和词语在各主题下的分布，这同样是基于特定算法的计算方式。</p><h4 id="ppl-计算依赖大模型能力" tabindex="-1">PPL 计算依赖大模型能力 <a class="header-anchor" href="#ppl-计算依赖大模型能力" aria-label="Permalink to &quot;PPL 计算依赖大模型能力&quot;">​</a></h4><p>在论文《Meta-Chunking: Learning Text Segmentation and Semantic Completion via Logical Perception》中，PPL 被用于文本分块任务，计算时要先将文本分割为句子序列，再用模型计算每个句子基于前文的 PPL 值。这里计算 PPL 所依赖的模型，实际上就是大语言模型（LLMs）。在实际应用中，大模型的训练过程是对大量文本数据中语言模式和规律的学习，其目的之一就是让模型在预测下一个词时更加准确，而 PPL 正是衡量这种预测准确性的指标。如果大模型没有学习到语言的逻辑和语义关系，就无法准确计算出反映句子间逻辑连贯性的 PPL 值。</p><h4 id="ppl-在大模型性能评估中的应用" tabindex="-1">PPL 在大模型性能评估中的应用 <a class="header-anchor" href="#ppl-在大模型性能评估中的应用" aria-label="Permalink to &quot;PPL 在大模型性能评估中的应用&quot;">​</a></h4><p>PPL 在评估大模型性能方面有重要作用。通过计算 PPL，可以了解大模型对文本的理解和预测能力。在模型训练过程中，PPL 可用于监控模型的收敛状态，帮助调整模型的训练参数。研究发现 PPL 与准确率存在强负相关性，可作为模型置信度指标，以此开发基于 PPL 的动态推理决策框架，在低置信度场景（PPL 超过阈值）下触发长文本推理，在高置信度场景直接输出简短答案，提升推理效率和准确性。这一系列应用都依赖于大模型对语言的理解和处理能力，PPL 作为衡量指标与之紧密结合。</p><p>在计算每个句子基于前文的 PPL 值时，模型（如论文中使用的 LLMs 或 SLMs）起到了核心的概率预测作用，是 PPL 值能够反映句子间逻辑连贯性的关键。具体来说，模型的作用体现在以下三个方面：</p><ol><li><p><strong>模型提供“条件概率预测”的能力</strong><br> PPL 的本质是通过模型对文本序列中“下一个 token”的预测概率来衡量句子间的逻辑关联，而这种概率预测完全依赖模型的语言理解能力。<br> 对于句子 $x_i$ 中的每个 token $t_k^i$，模型需要计算其在“前文所有信息（即 $t_{\\lt i}$，也就是 $x_i$ 之前的所有句子的 token）”和“该句子内部前文 token（$t_{\\lt k}^i$）”条件下的出现概率 $P(t_k^i | t_{\\lt k}^i, t_{\\lt i})$ 。<br> 例如，若前文讨论“动物迁徙的原因”，模型会基于已学习的语言规律，预测下一个 token 更可能是“气候”“食物”等相关词汇（概率高），而不是“计算机”“金融”等无关词汇（概率低）。这种预测能力直接来自模型对文本逻辑和语义的理解。</p></li><li><p><strong>模型的“逻辑感知能力”决定 PPL 对分块的指导意义</strong><br> 论文中明确提到，PPL Chunking 的设计核心是利用 LLMs 的逻辑感知能力，通过 PPL 值捕捉句子间的深层逻辑关系（如递进、并列、因果等），而非表面的语义相似性 。<br> 模型在训练过程中学习了文本中隐藏的逻辑结构（如“因为 A，所以 B”的因果关系会让模型对 B 的预测概率更高），因此其输出的 PPL 值能间接反映这种逻辑关联：</p><ul><li>当句子 $x_i$ 与前文逻辑连贯时，模型对 $x_i$ 中 token 的预测更“确定”（概率高），PPL 值低；</li><li>当 $x_i$ 与前文逻辑断裂时，模型预测更“困惑”（概率低），PPL 值高 。<br> 例如，对于句子序列“小明喜欢运动。他每天早上跑步。”，模型能识别两者的逻辑关联（“喜欢运动”是“跑步”的前提），因此第二句的 PPL 值较低；若改为“小明喜欢运动。天空是蓝色的。”，模型会因逻辑断裂而给出较高的 PPL 值。</li></ul></li><li><p><strong>模型规模与能力影响 PPL 计算的准确性</strong><br> 论文实验表明，不同规模的模型（如 Qwen2 - 0.5B、Qwen2 - 7B、Baichuan2 - 7B）计算的 PPL 值对分块效果有差异，这说明模型的能力直接影响 PPL 的可靠性 。</p><ul><li>能力更强的模型（如更大参数的 LLMs）能更好地理解复杂逻辑关系，其计算的 PPL 值更能准确区分“逻辑连贯”和“逻辑断裂”的边界；</li><li>即使是小规模模型（SLMs），通过微调后也能学习到基础的逻辑感知能力，从而计算出有效的 PPL 值，这也是论文强调“无需依赖强指令跟随能力的大模型”的原因 。</li></ul></li></ol><p>最后我们对上面关于PPL与模型关系做一个总结：模型只是计算 PPL 值的“核心工具”——它通过学习到的语言规律和逻辑感知能力，为每个 token 的出现概率提供预测（Mask掩码一样），而 PPL 值正是基于这些概率计算得出的。最终，PPL 值的高低能够反映句子间逻辑关联的强弱，本质上是模型对文本逻辑结构的“量化表达”，这也是 PPL 能用于确定分块边界的底层原因。</p>',33),s("p",null,[i("现在，我们通过公式来更加详细的了解PPL，首先，回忆一下分块的"),s("strong",null,"核心目标："),i(" 将长文本分割成多个"),s("strong",null,"连贯"),i("的语块 ("),s("code",null,"X1, X2, ..., Xk"),i(")，每个语块由连续的句子组成 ("),s("code",null,"xi"),i(")，并且最终语块的长度尽可能满足用户需求。 $$ \\text{PPL}"),s("em",{k:"1"},"M(x_i) = \\sum"),i("^{K} \\frac{\\text{PPL}"),s("em",null,"M(t_k^i ,|, t"),i("{\\lt k}^i, t_{\\lt i})}{K} $$ 我们来看懂这个公式：")],-1),a('<ul><li><p><strong>目标：</strong> 量化当前句子 $xi$ 在给定其<strong>之前所有文本内容</strong> ($x1, x2, ..., xi-1$) 的上下文下，语言模型 (LLM $M$) 对其的“惊讶程度”或“预测难度”。</p></li><li><p><strong>PPL:</strong> 困惑度 (Perplexity, PPL) 是衡量语言模型预测能力的一个指标。<strong>PPL 越低，表示模型对当前词序列的出现越不感到“困惑”，即该序列在给定上下文中越“自然”、越“可预测”、越“连贯”。</strong></p></li><li><p><strong>公式 (1) 详解：</strong> $$PPL_M(xi) = ( Σ_{k=1}^{K} PPL_M(t_k^i | t_{\\lt k}^i, t_{\\lt i}) ) / K$$</p><ul><li>$xi$: 当前要计算 PPL 的第 $i$ 个句子。</li><li>$K$: 句子 $xi$ 中包含的 <strong>token (词元)</strong> 的总数量。例如，$xi$ = “我喜欢苹果” 可能被分词成 <code>[&quot;我&quot;, &quot;喜欢&quot;, &quot;苹果&quot;]</code>，那么 $K=3$。</li><li>$t_k^i$: 句子 $xi$ 中的第 <code>k</code> 个 token。</li><li>$t_{\\lt k}^i$: 句子 $xi$ 中，排在 $t_k^i$ <strong>之前</strong>的所有 token。即 $xi$ 句子内部，<code>t_k^i</code> 的上文。</li><li>$t_{\\lt i}$: 所有排在句子 $xi$ <strong>之前</strong>的 token。即整个文档中，$xi$ 之前的所有句子的所有 token。这是 $xi$ 的<strong>外部上文</strong>，提供了最重要的语境。</li><li>$PPL_M(t_k^i | t_{\\lt k}^i, t_{\\lt i})$: <strong>核心条件概率计算。</strong> 这表示在给定以下两部分的条件下： <ul><li>$t_{\\lt k}^i$ (句子 $xi$ 内部，$t_k^i$ 之前的 token)</li><li>$t_{\\lt i}$ (整个文档中，$xi$ 之前的所有 token) 语言模型 <code>M</code> 预测出 token $t_k^i$ 的<strong>困惑度</strong>。更基础的计算通常是模型预测 $t_k^i$ 的<strong>概率</strong> $P_M(t_k^i | t_{\\lt k}^i, t_{\\lt i})$。PPL 是概率的衍生度量（通常基于概率的对数和平均）。</li></ul></li><li><strong>$Σ_{k=1}^{K} ... / K$</strong>: 对句子 $xi$ 中的 <strong>每一个 token (<code>k</code> 从 1 到 <code>K</code>)</strong> 计算其条件 PPL，然后将这 <code>K</code> 个值<strong>求和后再除以 <code>K</code> (句子 token 总数)</strong>。这得到了句子 $xi$ 在整个上下文 ($t_{\\lt i}$) 下的<strong>平均 token 级困惑度</strong>。<strong>这个平均值 $PPL_M(xi)$ 就代表了整个句子 $xi$ 在给定前文 ($t_{\\lt i}$) 下的连贯性/自然度分数。分数越低，句子 $xi$ 与前文的衔接越流畅、越符合预期。</strong></li></ul></li></ul><p>当我们计算完一个句子中，所有的token 的 PPL 后</p><p><img src="'+o+`" alt="alt text"></p><p>开始<strong>构建 PPL 序列 (PPL Sequence Construction):</strong></p><ul><li>处理：对序列 <code>S</code> 中的<strong>每一个句子 $xi$ (i 从 1 到 n)</strong> 都执行上文的PPL计算，计算其 $PPL_M(xi)$。</li><li>输出：得到一个与句子序列对应的 PPL 值序列：$PPL_seq = (PPL_M(x1), PPL_M(x2), ..., PPL_M(xn))$。</li><li><strong>关键：</strong> 当一个句子 $xi$ 与其前文 ($t_{\\lt i}$) 在语义或话题上发生<strong>转换或断裂</strong>时，模型预测 $xi$ 的难度会<strong>增大</strong>，导致 $PPL_M(xi)$ <strong>升高</strong>。相反，如果 $xi$ 与前文紧密相关，则 $PPL_M(xi)$ 会相对<strong>较低</strong>。更重要的是，在<strong>话题结束点或新话题开始点</strong>附近，我们预期会看到 $PPL_seq$ 中的<strong>局部极小值 (local minima)</strong>。这些极小值点标识了文本中相对<strong>更自然、更连贯的潜在分割位置</strong>。</li></ul><p>然后开始根据图<strong>识别关键分割点 (Finding Segmentation Points - Minima Detection):</strong></p><ul><li>目标：分析 $PPL_{seq}$，找出可以作为潜在语块边界的点（主要是局部极小值点）。</li><li><strong>算法关注两类极小值点 (对于点 <code>i</code>, 即句子 <code>xi</code> 的位置)：</strong><ul><li><strong>类型 1 (波谷):</strong> $PPL_M(xi-1) \\gt PPL_M(xi)$ <strong>且</strong> $PPL_M(xi+1) \\gt PPL_M(xi)$ <strong>且</strong> ($PPL_M(xi-1) - PPL_M(xi) \\gt θ$ <strong>或</strong> $PPL_M(xi+1) - PPL_M(xi) \\gt θ$)。 <ul><li>点 <code>i</code> 的 PPL 比它左边 (<code>i-1</code>) 和右边 (<code>i+1</code>) 邻居的 PPL 都低（形成一个谷底），并且<strong>至少有一侧</strong>的下降幅度（左邻居减当前值 或 右邻居减当前值）超过了预设的<strong>阈值 <code>θ</code></strong>。<code>θ</code> 是一个可调参数，用于过滤掉微小的、可能由噪声引起的波动，只关注显著的连贯性转换点。</li></ul></li><li><strong>类型 2 (左悬崖):</strong> $PPL_M(xi-1) - PPL_M(xi) \\gt θ$ <strong>且</strong> $PPL_M(xi+1) == PPL_M(xi)$。 <ul><li>：点 <code>i</code> 的 PPL 相比其<strong>左邻居 (<code>i-1</code>)</strong> 有一个<strong>显著的下降</strong>（下降幅度 &gt; <code>θ</code>），而其<strong>右邻居 (<code>i+1</code>)</strong> 的 PPL 与它<strong>持平</strong>。这通常表示一个话题在 $xi$ 处结束或发生剧变，而 $x_i$ 和 $x_{i+1}$ 可能属于同一个新单元的开头。</li></ul></li></ul></li><li>输出：一个潜在分割点位置的集合 <code>B = {b1, b2, ..., bm}</code> (例如 <code>b1=3</code>, <code>b2=7</code>, <code>b3=12</code> 表示建议在句子3后、7后、12后进行分割)。这些点标识了文本中相对连贯的子单元（元块）之间的边界。</li></ul><p><strong>生成元块 (Meta-Chunk Formation):</strong> 根据分割点集合 <code>B</code> 将句子序列 <code>S</code> 切分开。 最后输出元块序列 $MC = (M_{C1}, M_{C2}, ..., M_{Cp})$。</p><ul><li>每个元块 $M_{Cj}$ 由<strong>两个相邻分割点之间（或从开头到第一个分割点，或最后一个分割点到结尾）的所有连续句子</strong>组成。</li><li>例如，分割点 <code>B = {3, 7, 12}</code>, 句子总数 <code>n=15</code>，则： * $M_{C1} = (x1, x2, x3)$ (句子1到句子3) * $M_{C2} = (x4, x5, x6, x7)$ (句子4到句子7) * $M_{C3} = (x8, x9, x10, x11, x12)$ (句子8到句子12) * $M_{C4} = (x13, x14, x15)$ (句子13到句子15) * 下文中，我们将使用论文作者开源的Demo代码进行演示，你可以在运行的时候查看终端，会发现终端中输出了句子的分段编号。</li><li><strong>元块 $M_{Cj}$ 的特性：</strong> 算法认为每个元块内部的句子在给定的上下文（前文）下，具有相对较高的连贯性（由 PPL 极小值点标识其边界）。它们是<strong>基于文本自身语义和结构</strong>划分出的<strong>基础连贯单元</strong>。<strong>元块的长度是算法根据连贯性自动决定的，通常是不均匀的。</strong> 观察上面的$M_{Cj}$，会有一个问题，元块的长度是不均匀的，例如，元块$M_{C1}$的长度是3，元块$M_{C2}$的长度是4，元块$M_{C3}$的长度是5，元块$M_{C4}$的长度是3。我们并不希望这样，因为这样可以会出现某些元块的长度太长或者太短。因此，一种更好的方法是动态组合元块以满足长度需求。 <strong>动态组合元块以满足长度需求 (Dynamic Combination for Final Chunks):</strong></li><li><strong>目标：</strong> 用户通常对最终输出语块 ($X$) 有<strong>最大长度限制 $L$</strong> (例如，不超过 512 tokens)。元块 $MC_j$ 本身可能太短或太长。这一步将<strong>连续的元块合并</strong>，形成最终的语块 $X_k$，使得每个 $X_k$ 的 token 总数尽可能接近但不超过 $L$。</li><li><strong>处理 (贪心策略示例):</strong><ul><li>初始化：从第一个元块 $MC_1$ 开始。</li><li>当前合并组：<code>CurrentChunk = [MC1]</code>, <code>CurrentLength = len(MC1)</code> (token 数)。</li><li>遍历后续元块 ($MC_2$, $MC_3$, ... $MC_p$)： <ul><li>如果 <code>CurrentLength + len(MC_next)</code> $\\leq$ $L$： <ul><li>将 <code>MC_next</code> 加入 <code>CurrentChunk</code>。</li><li><code>CurrentLength = CurrentLength + len(MC_next)</code>。</li></ul></li><li>否则 (<code>CurrentLength + len(MC_next)</code> &gt; $L$)： <ul><li>将当前的 <code>CurrentChunk</code> 输出为一个最终语块 $X_k$。</li><li>重置 <code>CurrentChunk = [MC_next]</code>。</li><li>重置 <code>CurrentLength = len(MC_next)</code>。</li></ul></li></ul></li><li>处理完所有元块后，将剩余的 <code>CurrentChunk</code> 输出为最后一个 $X_k$。</li></ul></li><li><strong>输出：</strong> 最终语块序列 $X = (X_1, X_2, ..., X_k)$。 <ul><li>每个 $X_k$ 由一个或多个<strong>连续的元块 ($MC_j$)</strong> 组成。</li><li>每个 $X_k$ 的总 token 长度 $\\leq$ $L$ (用户设定的最大长度)。</li><li><strong>关键：</strong> 组合发生在<strong>元块的边界处</strong>。算法<strong>不会</strong>在一个元块 $MC_j$ 内部（即其连贯句子组内部）进行切割。<strong>优先保证了最终语块 $X_k$ 内部包含的是完整的、算法认为连贯的子单元 (元块)。</strong> 牺牲的是最终块可能由多个元块组成，但避免了破坏基础的连贯单元。</li></ul></li></ul><p>若文本超出 LLMs 或设备的处理范围，我们会策略性地引入键值（KV）缓存机制：先按 token 将文本分割为若干部分形成多个子序列，随着 PPL 计算的推进，当 GPU 内存即将超过服务器配置或 LLMs 最大上下文长度时，算法会适时移除先前部分文本的 KV 键值对，从而不过度牺牲上下文连贯性。</p><ul><li><strong>KV 缓存：</strong> Transformer 模型在计算注意力时会生成 Key (<code>K</code>) 和 Value (<code>V</code>) 矩阵，代表之前 token 的信息。缓存这些 <code>K</code> 和 <code>V</code> 可以避免在预测后续 token 时重新计算前面的所有层，大大加速自回归生成和 PPL 计算。</li><li><strong>滚动缓存策略 (Rolling Cache Strategy):</strong><ul><li>将整个文本按 token 分成多个<strong>子序列</strong> (Segment)。计算第一个子序列的 PPL 时，正常建立并保留其完整的 KV 缓存。当处理到后续子序列，并且<strong>累计的 KV 缓存大小接近设备 (GPU) 内存上限或模型最大上下文长度</strong>时：策略性地<strong>丢弃最早的一部分子序列的 KV 缓存</strong>。保留最近、最相关的部分子序列的 KV 缓存。用剩余缓存 + 新 token 继续计算后续句子的 PPL。</li></ul></li><li>丢弃部分历史缓存会损失一些远距离的上下文信息，可能轻微影响后续句子 PPL 计算的绝对准确性。但这种方法<strong>在内存限制和保持足够连贯性上下文之间取得了平衡</strong>，使得处理超长文本成为可能。通常，近期保留的上下文对于保持局部连贯性（即识别局部极小值点）已经足够重要。</li></ul><h2 id="边缘采样分块-margin-sampling-chunking" tabindex="-1">边缘采样分块（Margin Sampling Chunking） <a class="header-anchor" href="#边缘采样分块-margin-sampling-chunking" aria-label="Permalink to &quot;边缘采样分块（Margin Sampling Chunking）&quot;">​</a></h2><p>首先，为什么要提出边缘采样分块呢？</p><h3 id="技术背景与解决的问题" tabindex="-1">技术背景与解决的问题 <a class="header-anchor" href="#技术背景与解决的问题" aria-label="Permalink to &quot;技术背景与解决的问题&quot;">​</a></h3><p>传统RAG在处理长文本时，会首先将长文本切分为多个元块，然后对元块进行分块，最后将分块后的结果进行拼接得到最终的语块；但存在以下三种问题：</p><ol><li>依赖于特定格式文本生成</li><li>需要正则表达式提取分块结果</li><li>在小模型（&lt;7B参数）上性能下降明显</li></ol><p>MSP边缘采样分块策略直接分析模型输出的概率分布，无需格式约束，计算复杂度降低，适合场景资源受限的场景，与PPL分块形成互补，覆盖不同类型文本边界，下面我们直接来看公式：</p><p>$$\\text{Margin}_M(x_i) = P_M(y=k_1 | \\text{Prompt}(x_i, X&#39;)) - P_M(y=k_2 | \\text{Prompt}(x_i, X&#39;))$$</p><p>其中</p><ol><li>$K_1$、$K_2$表示分割判断时，对Yes/No做出的二元决策。其中Yes决策表示:模型认为&quot;应该分块&quot;的概率，同理No决策表示模型认为&quot;不应该分块&quot;的概率。</li><li>$\\text{Prompt}(x_i, X&#39;)$ 表示：分块判断提示，构造包括当前句 $x_i$ 和上下文 $X$ 的指令，如：判断句子 $x_i$ 是否应为前文 $X$ 的分块，回答Yes或者No。</li><li>$\\text{Margin}_M(x_i)$ 边缘概率差，模型对于两个分块决策的确定性度量，值越小表示决策越不稳定。换句话说，当模型对某段文本是否应该分块的判断变得模糊不清时，这种不确定性往往出现在语义边界处，此时应该考虑在该位置进行分块。</li></ol><p>随后，通过将 $\\text{Margin}_M(x_i)$ 与阈值 $\\theta$ 进行对比，就能得出这两个句子是否应被分割的结论。</p><h3 id="动态阈值机制" tabindex="-1">动态阈值机制 <a class="header-anchor" href="#动态阈值机制" aria-label="Permalink to &quot;动态阈值机制&quot;">​</a></h3><p>此外，为决策标准设定阈值是所有策略的普遍要求，为此我们引入了<strong>动态阈值机制</strong>。具体而言：</p><ul><li>在 $\\theta$ 的初始化阶段，我们为其赋予 0 的初始值，你可以设置为0。</li><li>随后，我们通过追踪历史 $\\text{Margin}_M(x_i)$ 值并计算其平均值来微调 $\\theta$，从而实现更灵活的分块调整。</li></ul><p>但是！单纯通过调整阈值控制分块大小可能导致随着阈值增大出现分块尺寸不均的问题，比如某些分块过长而另一些过短，这会影响后续检索的效果。为此，我们提出元分块与动态合并相结合的策略，旨在灵活应对不同分块要求。</p><p>首先，采用 PPL 分块或 MSP 分块将文档划分为若干元分块，记作 $(c_1, c_2, \\ldots, c_{\\alpha})$。传统分块方法将句子视为独立逻辑单元，而我们则将元分块作为独立逻辑单元。元分块是基于文本自身语义和结构划分出的基础连贯单元，内部句子具有相对较高的连贯性。</p><p>随后，根据用户指定的分块长度 $L$，迭代合并相邻元分块直至总长度满足或接近要求。具体而言，我们会按顺序尝试合并相邻的元分块。若当前合并的元分块总长度刚好等于 $L$，或者当前合并的元分块总长度小于 $L$，但再加入下一个元分块后总长度就会超过 $L$，则将当前合并的这些元分块视为一个完整分块。</p><p>例如，若满足 $$\\text{len}(c_1, c_2, c_3) = L$$ 或 $$\\text{len}(c_1, c_2, c_3) \\lt L$$ 且同时满足 $$\\text{len}(c_1, c_2, c_3, c_4) \\gt L$$，则将 $c_1, c_2, c_3$ 视为完整分块。</p><p>至此，我们得到了符合我们要求的（语义分割、块大小）数据块。</p><h2 id="全局信息补偿机制" tabindex="-1">全局信息补偿机制 <a class="header-anchor" href="#全局信息补偿机制" aria-label="Permalink to &quot;全局信息补偿机制&quot;">​</a></h2><p>然而，即使采用了上述精心设计的分块策略，文本分割过程仍然不可避免地会带来<strong>语义断裂问题</strong>。这种断裂主要表现在以下几个方面：</p><ul><li><strong>跨块引用断裂</strong>：当一个概念在块A中首次定义，但在块B中被引用时，块B的读者可能无法理解该引用的含义。例如，&quot;根据前述的马尔可夫假设...&quot;这样的表述在独立的文本块中会失去其指代对象。</li><li><strong>逻辑链条中断</strong>：复杂的论证过程往往跨越多个段落，当这些段落被分割到不同块中时，每个块的逻辑完整性都会受损。比如，&quot;因此可以得出...&quot;这样的结论性语句如果缺少前提条件，就会变得难以理解。</li><li><strong>上下文信息缺失</strong>：某些专业术语或概念需要特定的背景知识才能理解，当这些背景信息被分散到其他块中时，会影响当前块的可理解性。</li></ul><p>为解决这些根本性问题，我们提出了一种<strong>全局增强的文本重写与摘要生成机制</strong>。该机制的核心思想是：通过智能识别和补充缺失信息，修复分块过程中产生的语义鸿沟，同时为每个文本块生成包含全局上下文的增强摘要。</p><p>具体而言，我们利用 LLM 作为判别器来检测每个分块是否存在语义缺失，若存在则触发上述的重写流程，包括：</p><h3 id="三阶段语义补全流程" tabindex="-1">三阶段语义补全流程 <a class="header-anchor" href="#三阶段语义补全流程" aria-label="Permalink to &quot;三阶段语义补全流程&quot;">​</a></h3><h4 id="_1-缺失内容识别" tabindex="-1">1. 缺失内容识别 <a class="header-anchor" href="#_1-缺失内容识别" aria-label="Permalink to &quot;1. 缺失内容识别&quot;">​</a></h4><p>借助大语言模型（LLM），再结合预处理阶段找到的相关信息，对每个文本片段进行深入分析。这一步的主要任务是找出当前片段里缺失的前提条件、背景知识、相关事实或者结论。大语言模型要把缺失信息的地方都列出来，还得说清楚需要补充什么内容。</p><h4 id="_2-缺失信息精炼" tabindex="-1">2. 缺失信息精炼 <a class="header-anchor" href="#_2-缺失信息精炼" aria-label="Permalink to &quot;2. 缺失信息精炼&quot;">​</a></h4><p>在这一阶段，我们会对前一步检测出来的可能缺失的信息进行打分和筛选。这么做是为了避免添加一些无关或者错误的内容，保证补充信息的准确性。</p><h4 id="_3-缺失信息补全" tabindex="-1">3. 缺失信息补全 <a class="header-anchor" href="#_3-缺失信息补全" aria-label="Permalink to &quot;3. 缺失信息补全&quot;">​</a></h4><p>根据上一阶段确定的缺失位置和需要补充的信息，让大语言模型（LLM）把这些信息和当前的文本块整合在一起。最终目标是生成一个上下文连贯、读起来自然的新文本块，让不同文本块之间的信息能很好地融合。</p><h3 id="上下文感知摘要生成" tabindex="-1">上下文感知摘要生成 <a class="header-anchor" href="#上下文感知摘要生成" aria-label="Permalink to &quot;上下文感知摘要生成&quot;">​</a></h3><p>处理完这些缺失后，我们在上下文感知摘要生成对所有分块执行<strong>摘要生成</strong>以进一步提升召回率，为最终提升问答性能奠定坚实基础。</p><p>这部分的主要目的是为每个文本块生成简洁但包含全局信息的摘要，让文本块能更好地感知上下文。</p><h4 id="生成补充摘要" tabindex="-1">生成补充摘要 <a class="header-anchor" href="#生成补充摘要" aria-label="Permalink to &quot;生成补充摘要&quot;">​</a></h4><p>模型会利用全局信息，为目标文本块生成一个补充摘要。这么做是为了弥补文本块在分割过程中可能丢失的语篇背景和外部关联信息。</p><h4 id="生成局部摘要并融合" tabindex="-1">生成局部摘要并融合 <a class="header-anchor" href="#生成局部摘要并融合" aria-label="Permalink to &quot;生成局部摘要并融合&quot;">​</a></h4><p>模型会单独对文本块的内容进行概括，生成一个总结核心观点的局部摘要。然后把这个局部摘要和前面生成的补充摘要融合提炼，得到一个能从全局角度介绍文本块内容的增强版摘要。</p><p>下面的内容更多的是一些数学公式的证明和难以理解的知识，可以选择不看，看了其实效果也并不明显，我们希望能从这篇论文中学习到新的文本分割的策略，所以，请将该文档往下翻，直接看<a href="#demo">Demo</a>这一章节。</p><h3 id="模型训练与损失函数" tabindex="-1">模型训练与损失函数 <a class="header-anchor" href="#模型训练与损失函数" aria-label="Permalink to &quot;模型训练与损失函数&quot;">​</a></h3><p>为了让我们提出的改写和摘要生成功能更好用，我们按照上面说的流程，分别为这两个功能构建了 20,000 个训练数据样本。同时，我们选择对小型语言模型（SLM）的所有参数进行微调。</p><p>对于输入序列 $X$ 和目标输出序列 $Y=(y_1, y_2, \\ldots, y_T)$，损失函数定义如下： $$L = -\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T}\\log P(y_t|y_{\\lt t}, X;\\theta)$$</p><p>其中：</p><ul><li>$P(y_t|y_{\\lt t}, X;\\theta)$ 表示模型在给定输入 $X$ 和之前生成的内容 $y_{\\lt t}$ 时，预测出真实目标标记 $y_t$ 的概率</li><li>$\\theta$ 表示模型的参数</li><li>$N$ 表示一批次里的样本数量</li></ul><p>关于数据集构建和微调时超参数的具体配置，可以查看附录 C。</p><hr><h2 id="附录-c-语义补全详细流程" tabindex="-1">附录 C：语义补全详细流程 <a class="header-anchor" href="#附录-c-语义补全详细流程" aria-label="Permalink to &quot;附录 C：语义补全详细流程&quot;">​</a></h2><h3 id="c-1-语义补全的必要性" tabindex="-1">C.1 语义补全的必要性 <a class="header-anchor" href="#c-1-语义补全的必要性" aria-label="Permalink to &quot;C.1 语义补全的必要性&quot;">​</a></h3><p>当原文被分割为孤立文本块时，每个文本块可能丢失跨块上下文关联、全局结构连贯性或隐含逻辑关系，从而引发以下问题：</p><ul><li><strong>信息不完整</strong>：关键细节被截断或分散在多个文本块中（例如：某公式定义在块A，而公式应用却在块C）</li><li><strong>语义不连贯</strong>：文本块间的逻辑关系断裂（例如：原因陈述与结果分析被分割在不同块中）</li><li><strong>噪声干扰</strong>：无关内容被错误地包含在文本块中（例如：将举例说明与核心论点混合）</li></ul><p>通过采用全局增强的重写与摘要生成技术，我们能够为每个文本块补充缺失的全局信息，弥合语义断层，最终提升RAG系统的响应质量。</p><h3 id="c-2-语义补全实施步骤" tabindex="-1">C.2 语义补全实施步骤 <a class="header-anchor" href="#c-2-语义补全实施步骤" aria-label="Permalink to &quot;C.2 语义补全实施步骤&quot;">​</a></h3><h4 id="c-2-1-信息缺口识别阶段" tabindex="-1">C.2.1 信息缺口识别阶段 <a class="header-anchor" href="#c-2-1-信息缺口识别阶段" aria-label="Permalink to &quot;C.2.1 信息缺口识别阶段&quot;">​</a></h4><p>我们首先采用QwQ-32B模型的长推理模式（可处理超长文本的特殊模式），全面识别以下类型的信息缺口：</p><ul><li>明确引用但未定义的术语</li><li>需要前文信息才能理解的代词指代</li><li>跨章节的逻辑依赖关系</li><li>分散在不同位置的相关数据</li></ul><h4 id="c-2-2-补充信息过滤阶段" tabindex="-1">C.2.2 补充信息过滤阶段 <a class="header-anchor" href="#c-2-2-补充信息过滤阶段" aria-label="Permalink to &quot;C.2.2 补充信息过滤阶段&quot;">​</a></h4><p>使用ERNIE-3.5-128K模型对潜在补充内容进行评分与过滤，具体指标包括：</p><ul><li><strong>相关性</strong>：与当前文本块主题的关联程度</li><li><strong>必要性</strong>：缺失后是否影响理解</li><li><strong>简洁性</strong>：补充内容是否精炼</li></ul><h4 id="c-2-3-文本块融合阶段" tabindex="-1">C.2.3 文本块融合阶段 <a class="header-anchor" href="#c-2-3-文本块融合阶段" aria-label="Permalink to &quot;C.2.3 文本块融合阶段&quot;">​</a></h4><p>将精炼后的信息片段与当前文本块内容融合，生成既保持上下文连贯性又具备更高语义完整性的文本段落。融合策略包括：</p><ul><li><strong>前置补充</strong>：对需要背景知识的文本块，在开头补充必要上下文</li><li><strong>插入补充</strong>：对术语定义类缺口，在首次出现位置插入解释</li><li><strong>后置补充</strong>：对结果分析类文本块，在结尾补充相关结论</li></ul><h3 id="c-3-增强型摘要生成" tabindex="-1">C.3 增强型摘要生成 <a class="header-anchor" href="#c-3-增强型摘要生成" aria-label="Permalink to &quot;C.3 增强型摘要生成&quot;">​</a></h3><p>ERNIE-3.5-128K模型采用两阶段策略生成增强型摘要：</p><ol><li><strong>全局补充摘要</strong>：利用文档级信息为当前文本块生成补充说明</li><li><strong>局部核心摘要</strong>：提炼当前文本块自身的核心内容</li><li><strong>融合优化</strong>：将两类摘要精细融合，形成既能体现局部重点又包含全局关联的增强型摘要</li></ol><h3 id="c-4-训练数据构建" tabindex="-1">C.4 训练数据构建 <a class="header-anchor" href="#c-4-训练数据构建" aria-label="Permalink to &quot;C.4 训练数据构建&quot;">​</a></h3><p>通过LLM驱动的数据蒸馏管道，我们构建了高质量训练样本集：</p><ul><li>为语义补全模块构建20,000条实例</li><li>为摘要生成模块构建20,000条实例</li><li>所有样本均包含人工标注的质量评分</li></ul><p>这些数据为小语言模型(SLM)的全参数微调提供了关键指导信号，使我们的框架能够在高性能与轻量级部署之间取得平衡。</p><h3 id="c-5-效果验证" tabindex="-1">C.5 效果验证 <a class="header-anchor" href="#c-5-效果验证" aria-label="Permalink to &quot;C.5 效果验证&quot;">​</a></h3><p><img src="https://arxiv.org/html/2410.12788v3/extracted/6463559/pic/ppl_rewrite.jpg" alt="PPL分布变化趋势"></p><p><strong>图5</strong>：不同LLMs间原始文本块与改写文本块的PPL分布变化趋势</p><p>如图5所示，经过语义补全处理后（紫色线），各类模型的PPL值（困惑度）普遍低于原始文本块（橙色线），表明文本块的语义连贯性和可理解性得到显著提升。特别是在Qwen2-7B和Qwen2.5-7B模型上，PPL值降低了约30%，验证了语义补全策略的有效性。</p><h2 id="理论基础与数学证明-选学" tabindex="-1">理论基础与数学证明（选学） <a class="header-anchor" href="#理论基础与数学证明-选学" aria-label="Permalink to &quot;理论基础与数学证明（选学）&quot;">​</a></h2><p>在深入理解 PPL 分块方法之前，我们需要从信息论的基础开始，逐步构建完整的理论框架。本节将详细解释为什么要使用 PPL 来指导文本分块，以及为什么更长的文本块通常能带来更好的效果。 传统的文本分块方法（如固定长度分块、句子分块）往往忽略了语言的内在统计特性。它们可能在语义边界的中间进行切分，破坏了文本的连贯性。PPL 分块方法的核心思想是：<strong>利用语言模型对文本的&quot;理解程度&quot;来指导分块决策</strong>。</p><p>但这里有一个关键问题：<strong>为什么模型的&quot;理解程度&quot;可以用来指导分块？更长的上下文真的会让模型理解得更好吗？</strong> 这就需要我们从信息论的角度来寻找答案。</p><hr><h3 id="信息论基础-熵、交叉熵与困惑度-ppl" tabindex="-1">信息论基础：熵、交叉熵与困惑度 (PPL) <a class="header-anchor" href="#信息论基础-熵、交叉熵与困惑度-ppl" aria-label="Permalink to &quot;信息论基础：熵、交叉熵与困惑度 (PPL)&quot;">​</a></h3><p><strong>评估指标 (交叉熵 - H(P, Q))：</strong> 衡量概率分布 <code>Q</code> 逼近真实语言数据背后的经验分布 <code>P</code> 的效果，常用<strong>交叉熵</strong>： $$H(P, Q) = E_p[-\\log Q] = -\\sum_{x \\in \\mathcal{X}} P(x) \\log Q(x)$$</p><ul><li><strong>物理意义：</strong> 想象你要用一种编码方式来压缩文本。如果你的编码方式（模型Q）很好地理解了真实语言的规律（分布P），那么压缩效果就会很好，需要的存储空间就少。交叉熵就是衡量这个&quot;压缩效率&quot;的指标——数值越小，说明你的模型越能准确预测下一个词应该是什么，压缩效果越好。</li></ul><p><strong>交叉熵的分解：</strong> $$H(P, Q) = H(P) + D_{KL}(P || Q)$$</p><p>让我们用更通俗的语言来解释这个公式：</p><ol><li><p><strong>H(P) - 语言本身的复杂度：</strong></p><ul><li><strong>通俗理解：</strong> 想象你在玩猜词游戏，$H(P)$ 就是这个游戏本身的难度。比如中文比英文复杂，古文比现代文复杂，这个复杂度是语言本身决定的，不会因为你用什么模型而改变。</li><li><strong>举个例子：</strong> 如果下一个词可能是&quot;苹果&quot;、&quot;香蕉&quot;、&quot;橘子&quot;中的任意一个，且概率相等，那么这个位置的不确定性就很高。如果99%的概率是&quot;苹果&quot;，那么不确定性就很低。</li><li><strong>关键点：</strong> 这个值是<strong>固定的</strong>，就像考试题目的难度一样，不管你是学霸还是学渣，题目难度都不会变。</li></ul></li><li><p><strong>D_{\\text{\\1}}(P || Q)散度 - 模型的&quot;理解偏差&quot;：</strong></p><ul><li><strong>通俗理解：</strong> 这个值衡量的是你的模型（比如ChatGPT）对语言的理解与真实语言规律之间的差距。</li><li><strong>举个例子：</strong><ul><li>真实情况：在&quot;我喜欢吃___&quot;这个句子中，下一个词是&quot;苹果&quot;的概率是60%，是&quot;汽车&quot;的概率是0.1%</li><li>你的模型预测：下一个词是&quot;苹果&quot;的概率是40%，是&quot;汽车&quot;的概率是10%</li><li>那么$D_{KL}(P || Q)$就衡量了这种预测偏差有多大</li></ul></li><li><strong>关键点：</strong> 这个值可以通过训练来<strong>降低</strong>，就像学生通过学习可以提高成绩一样。</li></ul></li></ol><p><strong>为什么这个分解很重要？</strong></p><ul><li>因为语言的复杂度$H(P)$是固定的，所以<strong>训练模型的目标就是让模型的理解偏差$D_{KL}(P || Q)$越来越小</strong></li><li>当模型完美理解语言时，$D_{KL}(P || Q) = 0$，此时交叉熵$H(P, Q) = H(P)$，达到理论最优值</li><li>这就像考试一样：题目难度固定，你只能通过学习来减少答错的题目，最终目标是考满分</li></ul><p><strong>困惑度 (PPL) - 更直观的评估指标：</strong></p><ul><li><strong>定义：</strong> 困惑度定义为交叉熵的指数形式： $$\\text{PPL}(P, Q) = 2^{H(P, Q)}$$</li><li><strong>直观解释：</strong> PPL 可以理解为模型在预测下一个词时，平均面临的<strong>等效选择数量</strong>。 <ul><li><em>例子：</em> 如果 PPL = 100，意味着模型在预测时，感觉平均有 100 个词看起来&quot;差不多可能&quot;会是下一个词。值越小越好。</li></ul></li><li><strong>PPL 与 KL 散度的关系：</strong> $$\\text{PPL}(P, Q) = 2^{H(P, Q)} = 2^{H(P) + D_{KL}(P || Q)}$$ <ul><li>因为 $H(P)$ 是常数，<strong>不同 LLM 之间的 PPL 差异主要由它们对应的 $D_{KL}(P || Q)$ 差异决定</strong>。KL 散度越大（模型越不准），PPL 越高。</li></ul></li><li>文本中 <strong>PPL 异常高的点</strong>通常意味着模型在该位置非常&quot;困惑&quot;，预测能力骤降。这往往发生在<strong>语义边界</strong>（如句子结束、段落结束、话题转折）。强行在这些高 PPL 点切分文本块，会破坏上下文完整性，导致后续处理（如 RAG 中的检索或问答）效果变差。因此，分块策略应<strong>避免在高 PPL 点切割</strong>，可以看上文中所提及的图表，正确切割点应该为谷底点，该点所对应的PPL值是特定区间中的极小值，代表模型对该位置&quot;不困惑&quot;，预测能力比较高。</li></ul><hr><h3 id="熵的近似-香农的-g-text-1-公式及其证明-核心重点" tabindex="-1">熵的近似：香农的 G_{\\text{\\1}} 公式及其证明 (核心重点) <a class="header-anchor" href="#熵的近似-香农的-g-text-1-公式及其证明-核心重点" aria-label="Permalink to &quot;熵的近似：香农的 G_{\\text{\\1}} 公式及其证明 (核心重点)&quot;">​</a></h3><p><strong>核心问题：</strong> 如何估计真实语言分布 <code>P</code> 的熵 $H(P)$？</p><p>让我们用一个简单的例子来理解这个问题：</p><p><strong>问题的本质：</strong> 想象你要预测一篇文章的下一个字。如果你只看当前这个字，预测会很困难；如果你看前面2个字，预测会容易一些；如果你看前面10个字，预测会更准确。</p><p><strong>香农的解决方案：</strong> 香农发现了一个规律：<strong>看的上下文越长，预测下一个字就越容易</strong>。他用数学公式把这个规律表达出来，这就是著名的G_{\\text{\\1}}公式。</p><p><strong>通俗理解：</strong></p><ul><li>G_{\\text{\\1}}就是&quot;看前面K个字，预测下一个字的平均难度&quot;</li><li>K越大（看的上下文越长），G_{\\text{\\1}}越小（预测越容易）</li><li>这个规律适用于所有自然语言</li></ul><p><strong>为什么这很重要？</strong> 这告诉我们：在文本分块时，<strong>保留更长的上下文信息会让AI模型理解得更好</strong>。这就是为什么我们要用PPL（困惑度）来指导分块——它能帮我们找到既保持语义完整，又不破坏上下文连贯性的最佳分割点。</p><ul><li><p><strong>G_{\\text{\\1}} 的定义：</strong> $$G_K = \\frac{1}{K} \\sum_{k=1}^{K} H(X_k | X_1, ..., X_{k-1}) \\quad \\text{(公式 6 的等价形式)}$$</p><ol><li>拿一段长度为 <code>K</code> 的文本（比如K=100个字）。</li><li>对于这段文本中的每一个位置 <code>k</code>（第1个字、第2个字、...、第100个字），计算&quot;知道前面所有字的情况下，预测这个位置的字有多难&quot;。</li><li>把这100个&quot;预测难度&quot;加起来，再除以100，得到平均难度。</li><li>这个平均难度 $G_K$ 就代表了：<strong>如果我们的记忆力只能记住前面K-1个字，那么预测下一个字的平均难度是多少</strong>（请记住这一句~）。</li></ol><ul><li>如果K=3，文本是&quot;我喜欢&quot;</li><li>$H(X_1)$：预测第1个字&quot;我&quot;的难度（没有任何提示）</li><li>$H(X_2|X_1)$：知道&quot;我&quot;的情况下，预测第2个字&quot;喜&quot;的难度</li><li>$H(X_3|X_1,X_2)$：知道&quot;我喜&quot;的情况下，预测第3个字&quot;欢&quot;的难度</li><li>$G_3 = \\frac{1}{3}[H(X_1) + H(X_2|X_1) + H(X_3|X_1,X_2)]$</li></ul></li></ul><p>下面我们将证明$G_{K} \\geq G_{K+1}$这个公式，首先你需要知道定理一：</p><ul><li><p><strong>定理 1 (G_{\\text{\\1}} 是 H_{\\text{\\1}}(P) 的上界)</strong></p></li><li><p><strong>定理 2 (G_{\\text{\\1}} 单调递减且有下界)</strong></p></li></ul><p>下面我们来证明定理二： $$G_1 \\geq G_2 \\geq G_3 \\geq \\cdots \\geq H_{rate}(P) \\quad \\text{(公式 8)}$$ * <strong>证明思路 (核心)：</strong> 关键在于证明 $G_{K} \\geq G_{K+1}$。 写出 $G_{K+1}$ 和 $G_K$： $$G_{K+1} = \\frac{1}{K+1} \\sum_{k=1}^{K+1} H(X_k | X_1, ..., X_{k-1})$$ $$G_K = \\frac{1}{K} \\sum_{k=1}^{K} H(X_k | X_1, ..., X_{k-1})$$ 将 $G_{K+1}$ 拆分为前 K 项和第 K+1 项： $$(K+1)G_{K+1} = \\sum_{k=1}^{K} H(X_k | X_1, ..., X_{k-1}) + H(X_{K+1} | X_1, ..., X_K)$$ 注意到 $\\sum_{k=1}^{K} H(X_k | X_1, ..., X_{k-1}) = KG_K$ 代入得： $$(K+1)G_{K+1} = KG_K + H(X_{K+1} | X_1, ..., X_K)$$ 由定理 1 的证明思路或直接由条件熵性质，我们知道：<strong>给模型提供更多的信息，预测就会更准确（或至少不会变差）</strong>。</p><p>具体来说：</p><ul><li>$H(X_{K+1} | X_1, ..., X_K)$：知道前面K个词的情况下，预测第K+1个词的难度</li><li>$H(X_{K+1} | X_2, ..., X_K)$：知道前面K-1个词的情况下，预测第K+1个词的难度</li></ul><p>显然，知道的信息越多（包括$X_1$），预测就越容易，所以： $$H(X_{K+1} | X_1, ..., X_K) \\leq H(X_{K+1} | X_2, ..., X_K)$$</p><p>进一步地，利用数学上的链式法则和熵的凹性质，可以证明一个更强的结论： $$H(X_{K+1} | X_1, ..., X_K) \\leq \\frac{1}{K} \\sum_{m=1}^{K} H(X_{K+1} | X_m, ..., X_K)$$ 香农发现了一个重要规律：<strong>对于平稳的语言序列，预测的难度只取决于你能看到的上下文长度，而不取决于这段文字出现在文章的哪个位置</strong>。</p><p>具体来说：</p><ul><li>$H(X_{K+1} | X_m, ..., X_K)$ 表示：已知第m个词到第K个词，预测第K+1个词的难度</li><li>上下文长度 = $(K - m + 1)$ 个词</li></ul><p>香农发现：只要上下文长度相同，预测难度就相同，不管这段上下文出现在文章开头、中间还是结尾。举个例子：看3个词预测第4个词的难度，在文章任何位置都是一样的；看5个词预测第6个词的难度，在文章任何位置也都是一样的。这样是不是更加清晰了。</p><p>因此，我们可以用一个简单的符号 $h_l$ 来表示&quot;看l个词预测下一个词的难度&quot;，其中 $l$ 就是上下文长度。这样就把复杂的位置相关问题，简化成了只与长度相关的问题。</p><h3 id="证明-g-k-geq-g-k-1-单调递减性" tabindex="-1">证明 $G_K \\geq G_{K+1}$（单调递减性） <a class="header-anchor" href="#证明-g-k-geq-g-k-1-单调递减性" aria-label="Permalink to &quot;证明 $G_K \\geq G_{K+1}$（单调递减性）&quot;">​</a></h3><p>根据平稳性假设，我们可以用 $h_l$ 表示&quot;看 $l$ 个词预测下一个词的难度&quot;：</p><p>$$G_K = \\frac{1}{K} (h_0 + h_1 + ... + h_{K-1})$$</p><p>$$G_{K+1} = \\frac{1}{K+1} (h_0 + h_1 + ... + h_K)$$</p><p>其中：</p><ul><li>$h_0 = H(X_1)$（无上下文）</li><li>$h_1 = H(X_2|X_1)$（上下文长度1）</li><li>$h_{K-1} = H(X_K|X_1, ..., X_{K-1})$（上下文长度K-1）</li><li>$h_K = H(X_{K+1}|X_1, ..., X_K)$（上下文长度K）</li></ul><p><strong>核心推导：</strong></p><p>从定义可得： $$(K+1)G_{K+1} = KG_K + h_K$$</p><p>移项得到： $$G_K - G_{K+1} = \\frac{G_K - h_K}{K+1}$$</p><p><strong>关键引理：</strong> 由于条件熵的性质，更长的上下文总是降低预测不确定性，因此： $$h_0 \\geq h_1 \\geq h_2 \\geq ... \\geq h_K \\geq ... \\geq H_{rate}(P)$$</p><p><strong>证明 $G_K \\geq h_K$：</strong> 由于 $h_0 \\geq h_1 \\geq ... \\geq h_{K-1}$，它们的平均值必然大于等于最小项： $$G_K = \\frac{1}{K} (h_0 + h_1 + ... + h_{K-1}) \\geq h_{K-1}$$</p><p>结合 $h_{K-1} \\geq h_K$，得到： $$G_K \\geq h_{K-1} \\geq h_K$$</p><p><strong>结论：</strong> $$G_K - G_{K+1} = \\frac{G_K - h_K}{K+1} \\geq 0$$</p><p>因此 $G_K \\geq G_{K+1}$，证明了序列的单调递减性。</p><p><strong>定理3（极限存在且等于熵率）：</strong> $$\\lim_{K \\to \\infty} G_K = H_{rate}(P)$$</p><p>由单调有界数列必收敛准则，结合熵率定义可得此结论。</p><p>最后，综合以上的公式定理，我们可以知道<strong>G_{\\text{\\1}} 公式的重要性与意义</strong>，该公式通过计算有限长度 <code>K</code> 下的 $G_K$，来估计语言的熵率。<strong>$G_K \\downarrow$ 意味着预测能力 $\\uparrow$：$G_K$ 越小，预测越准确。</strong> 其次，<strong>上下文长度是关键 ($K \\uparrow \\Rightarrow G_K \\downarrow$)，</strong> 根据 定理 2 ($G_1 \\geq G_2 \\geq ...$) 表明，<strong>提供更长的历史上下文 (<code>K</code> 增大) 会显著降低预测下一个符号的平均不确定性 ($G_K$ 减小)</strong>。 所以对于PPL来说：<strong>给 LLM 提供更长的上下文 (<code>K</code> 增大)，其预测下一个词的条件交叉熵也会降低</strong>。进而，模型在整个长序列上的<strong>平均交叉熵 $H(P, Q)$ 和困惑度 PPL 也会降低</strong>。</p><p>总结就是下面一句话： $$\\text{更长的上下文 } (K \\uparrow) \\implies \\text{更低的预测不确定性} \\implies \\text{更低的 } H(P, Q) \\implies \\text{更低的 PPL}$$</p><h2 id="demo" tabindex="-1">Demo <a class="header-anchor" href="#demo" aria-label="Permalink to &quot;Demo&quot;">​</a></h2><p>下面将根据论文以及作者的<a href="https://github.com/IAAR-Shanghai/Meta-Chunking" target="_blank" rel="noreferrer">开源项目</a>复现：<a href="./../project/Meta_chunking/README.html">Meta-chunking</a></p><p>首先，打开modelscope的官网，注册账号，并登录。打开自己的notebook,GPU/CPU无所谓，使用下面的指令安装conda</p><div class="language-shell vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">wget</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">chmod</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> +x</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> Miniconda3-latest-Linux-x86_64.sh</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>Miniconda 允许用户在安装过程中指定安装路径。当您下载 Miniconda 安装包后，在运行安装命令时可以通过 -p 或 --prefix 参数来指定安装目录。我们要确保将其安装到Notebook的默认存储路径/mnt/workspace下，因为只有这个路径下的文件，才会被持久化保存。</p><p>具体来说，假如我们希望将其安装到<code>/mnt/worksapce/miniconda3 </code>目录下，可使用如下命令：</p><div class="language-shell vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">!</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">bash</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> Miniconda3-latest-Linux-x86_64.sh</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -b</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /mnt/workspace/miniconda3</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>-p指定的是您希望安装 Miniconda 的完整路径。 -b表示以批处理模式运行安装程序，不需要在安装过程提示交互。</p><p>初始化Conda 手工指定安装路径时，在安装后需要显式地调用conda binary 进行初始化：</p><div class="language-shell vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">/mnt/workspace/miniconda3/bin/conda</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> init</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>初始化成功以后，关闭当前的terminal窗口，再重新打开。就可以正常使用conda命令了。</p><p>请注意，在您关闭ModelScope Notebook实例后，后续每次打开ModelScope Notebook的时候，都需要重新执行一次如上的初始化命令。</p><p>在你的notebook中，执行如下指令</p><div class="language-shell vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://github.com/IAAR-Shanghai/Meta-Chunking.git</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">cd</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> Meta-Chunking</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">conda</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> create</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> MetaChunking</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> python=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3.10</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">conda</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> activate</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> MetaChunking</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -r</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> requirements.txt</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 这一步pip install非常慢，因为需要下载大量的包</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">cd</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> example</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> app.py</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>如果报错，出现</p><div class="language-shell vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">OSError:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> We</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> couldn&#39;t connect to &#39;https://huggingface.co&#39; to load this file, couldn&#39;t</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> find</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> it</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> in</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> the</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cached</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> files</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> and</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> it</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> looks</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> like</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> Qwen2-1.5B-Instruct</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> is</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> not</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> the</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> path</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> to</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> a</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> directory</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> containing</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> a</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> file</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> named</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> config.json.</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Checkout</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> your</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> internet</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> connection</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> or</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> see</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> how</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> to</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> the</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> library</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> in</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> offline</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> mode</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> at</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;https://huggingface.co/docs/transformers/installation#offline-mode&#39;.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>请打开example/app.py文件，将</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> gradio </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> gr</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> transformers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> json</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.nn.functional </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> F</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model_name_or_path</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;Qwen2-1.5B-Instruct&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">   </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">device_map </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;auto&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">small_tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoTokenizer.from_pretrained(model_name_or_path,</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">trust_remote_code</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">small_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(model_name_or_path, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">trust_remote_code</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device_map</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">device_map) </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">small_model.eval()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>修改为</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> gradio </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> gr</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modelscope </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoModelForCausalLM, AutoTokenizer  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 使用ModelScope的Auto类</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> json</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.nn.functional </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> F</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model_name_or_path </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;qwen/Qwen2-1.5B-Instruct&#39;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # ModelSpace路径</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">device_map </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;auto&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 使用ModelScope的AutoTokenizer</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">small_tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoTokenizer.from_pretrained(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model_name_or_path,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    trust_remote_code</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 使用ModelScope的AutoModel</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">small_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model_name_or_path,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    trust_remote_code</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    device_map</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">device_map,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    torch_dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.bfloat16  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 添加数据类型以节省显存</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">small_model.eval()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><p>注意：首次运行需要安装ModelSpace客户端</p><div class="language-shell vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> modelscope</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>最后我们将看到如下的效果： <img src="`+h+'" alt="alt text"></p><p>是否成功看到这个页面了呢？下面将讲解代码逻辑，看看作者是如何通过代码来实现PPL以及MSP的。</p>',159)])])}const F=t(k,[["render",g]]);export{m as __pageData,F as default};
